{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme (EDITED BY JOHN and Guwei)\n",
        "\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "from sentence_transformers import CrossEncoder\n",
        "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "def retrieve(evi_ebds, claim_ebds, evi_df, claim_df, retrival_top_k, rerank_top_k,threshold_activated,score_threshold, cross_encoder,dev):\n",
        "    embedding_dim = evi_ebds.shape[1]\n",
        "    index = faiss.IndexFlatL2(embedding_dim)\n",
        "    faiss.normalize_L2(evi_ebds)\n",
        "    index.add(evi_ebds)\n",
        "    retrieval = pd.DataFrame()\n",
        "    retrieved_labels = []\n",
        "    retrieved_evidences = []\n",
        "\n",
        "    i = 0\n",
        "    counts = 0\n",
        "    claim_texts  =[]\n",
        "    total = len(claim_ebds)\n",
        "    for dev_claim_embedding in claim_ebds: \n",
        "        faiss.normalize_L2(dev_claim_embedding.reshape(1, -1)) \n",
        "        D, I = index.search(dev_claim_embedding.reshape(1, -1), retrival_top_k)\n",
        "        text = claim_df.iloc[i]['claim_text']\n",
        "        pairs = [(text, evi_df['value'][a]) for a in I[0]]\n",
        "        scores = cross_encoder.predict(pairs)\n",
        "        reranked = sorted(zip(I[0], scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        retrieved_evidence = []\n",
        "        filtered = []\n",
        "        first = True\n",
        "        for idx, score in reranked[:rerank_top_k]:\n",
        "            if first:\n",
        "                evi_id = evi_df['ID'][idx]\n",
        "                evi_content = evi_df['value'][idx]\n",
        "                retrieved_evidence.append(evi_id)\n",
        "                filtered.append((evi_id, score, evi_content))\n",
        "                first = False\n",
        "            else:\n",
        "                if not threshold_activated or score >= score_threshold:\n",
        "                    evi_id = evi_df['ID'][idx]\n",
        "                    evi_content = evi_df['value'][idx]\n",
        "                    retrieved_evidence.append(evi_id)\n",
        "                    filtered.append((evi_id, score, evi_content))\n",
        "\n",
        "        retrieved_evi =  [evi_df['ID'][a] for a in I[0]]\n",
        "        retrieved_evidences.append(retrieved_evidence)\n",
        "        retrieved_labels.append('SUPPORTS')\n",
        "        claim_texts.append(text)\n",
        "\n",
        "        if dev:\n",
        "            print(f\"Claim: {text}\")\n",
        "            if len(filtered) > 0:\n",
        "                print(\"evidence relevant\")\n",
        "                for eid, score, c in filtered:\n",
        "                    print(f\"  {eid}, Score: {score:.4f} \")\n",
        "            print(f\"Ground truth: {claim_df.iloc[i]['evidences']}\\n\")\n",
        "\n",
        "            count = 0\n",
        "            for g in claim_df.iloc[i]['evidences']:\n",
        "                if g in retrieved_evi:\n",
        "                    count += 1\n",
        "            counts += count / len(claim_df.iloc[i]['evidences'])\n",
        "\n",
        "\n",
        "\n",
        "        print('Progress ', round(i * 100 / total, 3), '%')\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    print(\"R: \", counts/ i)\n",
        "    retrieval['ID'] = claim_df['ID']\n",
        "    retrieval['evidences'] = retrieved_evidences\n",
        "    retrieval['claim_label'] = retrieved_labels\n",
        "    retrieval['claim_text'] = claim_texts\n",
        "    return retrieval\n",
        "\n",
        "def mine_hard_negatives(retrieved_evidences, ground_truth_evidences):\n",
        "    negative_evidences = []\n",
        "    for i in range(len(retrieved_evidences)):\n",
        "        retrieved_evidence = retrieved_evidences.iloc[i]\n",
        "        ground_truth_evidence = ground_truth_evidences.iloc[i]\n",
        "        negative_evidence = []\n",
        "        for e in retrieved_evidence:\n",
        "            if e not in ground_truth_evidence:\n",
        "                negative_evidence.append(int(re.findall(r'\\d+', e)[0]))\n",
        "        negative_evidences.append(negative_evidence)\n",
        "    return negative_evidences\n",
        "\n",
        "def load_train_data(train_dataframe, evidence_dateframe, random_negatives_amount_per_claim):\n",
        "    train_data = []\n",
        "    for id in range(len(train_dataframe)):\n",
        "        claim_text =  train_dataframe.iloc[id]['claim_text']\n",
        "        # print(\"CLAIM: \", claim_text)\n",
        "        positive_evidence_ids = train_dataframe.iloc[id]['evidences_numeric_index']\n",
        "        negative_evidence_ids = train_dataframe.iloc[id]['negative_evidences']\n",
        "        for evid_id in positive_evidence_ids:\n",
        "            \n",
        "            evidence_text = evidence_dateframe.iloc[evid_id]['value']\n",
        "            # print(\"POSITIVE: \", evidence_text)\n",
        "            train_data.append(InputExample(texts=[claim_text, evidence_text], label=1.0))\n",
        "        for ngevid_id in negative_evidence_ids:\n",
        "            evidence_text = evidence_dateframe.iloc[ngevid_id]['value']\n",
        "            # print(\"NEGATIVE: \", evidence_text)\n",
        "            train_data.append(InputExample(texts=[claim_text, evidence_text], label=0.0))\n",
        "\n",
        "        for i in range(random_negatives_amount_per_claim):\n",
        "            neg_id = random.choice(list(set(evidence_dateframe.index) - set(positive_evidence_ids)))\n",
        "            neg_text = evidence_dateframe.iloc[neg_id]['value']\n",
        "            train_data.append(InputExample(texts=[claim_text, neg_text], label=0.0))\n",
        "    return train_data\n",
        "\n",
        "def load_positive_train_data(train_dataframe, evidence_dateframe):\n",
        "    train_data = []\n",
        "    for id in range(len(train_dataframe)):\n",
        "        claim_text =  train_dataframe.iloc[id]['claim_text']\n",
        "        positive_evidence_ids = train_dataframe.iloc[id]['evidences_numeric_index']\n",
        "        for evid_id in positive_evidence_ids:\n",
        "            evidence_text = evidence_dateframe.iloc[evid_id]['value']\n",
        "            train_data.append(InputExample(texts=[claim_text, evidence_text], label=1.0))\n",
        "    return train_data\n",
        "\n",
        "def save_retrieval(retrieval, path):\n",
        "    output = {}\n",
        "    for i in range(len(retrieval)):\n",
        "        output[retrieval.iloc[i]['ID']] = {}\n",
        "        output[retrieval.iloc[i]['ID']]['evidences'] = retrieval.iloc[i]['evidences']\n",
        "        output[retrieval.iloc[i]['ID']]['claim_label'] = retrieval.iloc[i]['claim_label']\n",
        "        output[retrieval.iloc[i]['ID']]['claim_text'] = ''\n",
        "\n",
        "    with open(path, 'w') as file:\n",
        "        file.write(json.dumps(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SLYK-guqEbP"
      },
      "source": [
        "Read evidence dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr4_3r4SrGtO"
      },
      "outputs": [],
      "source": [
        "with open('evidence.json', 'r') as f:\n",
        "    evidence = json.load(f)\n",
        "flat_list = []\n",
        "for key in evidence:\n",
        "    flat_list.append({\"ID\": key, \"value\": evidence[key]})\n",
        "\n",
        "evidence_df = pd.DataFrame(flat_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read training and developing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "with open('train-claims.json', 'r') as f:\n",
        "    train_df = json.load(f)\n",
        "flat_list = []\n",
        "for key in train_df:\n",
        "    evidences_numeric_index = []\n",
        "    for e in train_df[key]['evidences']:\n",
        "        evidences_numeric_index.append(int(re.findall(r'\\d+', e)[0]))\n",
        "    flat_list.append({\"ID\": key, \"claim_text\": train_df[key]['claim_text'], \"claim_label\": train_df[key]['claim_label'], \"evidences\": train_df[key]['evidences'], \"evidences_numeric_index\": evidences_numeric_index})\n",
        "train_df = pd.DataFrame(flat_list)\n",
        "\n",
        "with open('dev-claims.json', 'r') as f:\n",
        "    dev_df = json.load(f)\n",
        "flat_list = []\n",
        "for key in dev_df:\n",
        "    evidences_numeric_index = []\n",
        "    for e in dev_df[key]['evidences']:\n",
        "        evidences_numeric_index.append(int(re.findall(r'\\d+', e)[0]))\n",
        "    flat_list.append({\"ID\": key, \"claim_text\": dev_df[key]['claim_text'], \"claim_label\": dev_df[key]['claim_label'], \"evidences\": dev_df[key]['evidences'], \"evidences_numeric_index\": evidences_numeric_index})\n",
        "dev_df = pd.DataFrame(flat_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate sentence embeddings for evidence dataset and trainning dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device) \n",
        "\n",
        "train_data = load_positive_train_data(train_df, evidence_df)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=10,\n",
        "    output_path='./output-bi-encoder',\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "evidence_embeddings = model.encode(evidence_df['value'], batch_size=64, show_progress_bar=True)\n",
        "dev_claims_embeddings = model.encode(dev_df['claim_text'], batch_size=64, show_progress_bar=True)\n",
        "train_claims_embeddings = model.encode(train_df['claim_text'], batch_size=64, show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine tune cross encoder for re-ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "def load_positive_train_ds(train_dataframe, evidence_dateframe, t_ebds, e_ebds):\n",
        "    query = []\n",
        "    answer = []\n",
        "    for id in range(len(train_dataframe)):\n",
        "        claim_text =  train_dataframe.iloc[id]['claim_text']\n",
        "        positive_evidence_ids = train_dataframe.iloc[id]['evidences_numeric_index']\n",
        "        # claim_ebd = t_ebds[id]\n",
        "        # none_added = True\n",
        "        for evid_id in positive_evidence_ids:\n",
        "            \n",
        "            # evi_ebd = e_ebds[evid_id]\n",
        "            evidence = evidence_dateframe.iloc[evid_id]['value']\n",
        "            # similarities = model.similarity(claim_ebd, evi_ebd)\n",
        "\n",
        "            # if similarities > 0:\n",
        "            #     none_added = False\n",
        "            # print('Claim: ', claim_text)\n",
        "            # print('Evidence: ', evidence)\n",
        "            \n",
        "            query.append(claim_text)\n",
        "            answer.append(evidence)\n",
        "        # if none_added:\n",
        "        #     query.append(claim_text)\n",
        "        #     answer.append(evidence_dateframe.iloc[positive_evidence_ids[0]]['value'])\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "    \"query\": query,\n",
        "    \"answer\": answer,})\n",
        "\n",
        "\n",
        "train_dataset = load_positive_train_ds(train_df, evidence_df, train_claims_embeddings, evidence_embeddings)\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers.cross_encoder.evaluation import CrossEncoderRerankingEvaluator\n",
        "from sentence_transformers.cross_encoder import CrossEncoderTrainingArguments, CrossEncoderTrainer, losses\n",
        "from sentence_transformers.util import mine_hard_negatives\n",
        "\n",
        "eval_dataset = load_positive_train_ds(dev_df, evidence_df,dev_claims_embeddings, evidence_embeddings)\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
        "hard_eval_dataset = mine_hard_negatives(\n",
        "    eval_dataset,\n",
        "    embedding_model,\n",
        "    corpus = train_dataset[\"answer\"],  # Use the full dataset as the corpus\n",
        "    num_negatives=50,  # How many negatives per question-answer pair\n",
        "    batch_size=4096,  # Use a batch size of 4096 for the embedding model\n",
        "    output_format=\"n-tuple\",  # The output format is (query, positive, negative1, negative2, ...) for the evaluator\n",
        "    include_positives=True,  # Key: Include the positive answer in the list of negatives\n",
        "    use_faiss=True,  # Using FAISS is recommended to keep memory usage low (pip install faiss-gpu or pip install faiss-cpu)\n",
        ")\n",
        "\n",
        "reranking_evaluator = CrossEncoderRerankingEvaluator(\n",
        "    samples=[\n",
        "        {\n",
        "            \"query\": sample[\"query\"],\n",
        "            \"positive\": [sample[\"answer\"]],\n",
        "            \"documents\": [sample[column_name] for column_name in hard_eval_dataset.column_names[2:]],\n",
        "        }\n",
        "        for sample in hard_eval_dataset\n",
        "    ],\n",
        "    batch_size=32,\n",
        "    name=\"gooaq-dev\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "reranking_evaluator(cross_encoder)\n",
        "\n",
        "\n",
        "model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "short_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
        "run_name = f\"reranker-{short_model_name}-gooaq-cmnrl\"\n",
        "args = CrossEncoderTrainingArguments(\n",
        "    # Required parameter:\n",
        "    output_dir=f\"models/{run_name}\",\n",
        "    # Optional training parameters:\n",
        "    num_train_epochs= 40,\n",
        "    per_device_train_batch_size= 16,\n",
        "    per_device_eval_batch_size= 16,\n",
        "    learning_rate=1e-6,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
        "    # Optional tracking/debugging parameters:\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    save_total_limit=2,\n",
        "    run_name=run_name,  # Will be used in W&B if `wandb` is installed\n",
        "    seed=12,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    weight_decay = 0.01,\n",
        ")\n",
        "loss = losses.MultipleNegativesRankingLoss(model=cross_encoder)\n",
        "\n",
        "trainer = CrossEncoderTrainer(\n",
        "    model=cross_encoder,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=reranking_evaluator,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# cross_encoder.fit(\n",
        "\n",
        "#     train_dataloader=train_dataloader,\n",
        "#     # loss_fct=train_loss,\n",
        "#     epochs=20,\n",
        "#     output_path='./fine-tuned-cross-encoder',\n",
        "#     weight_decay=0.01\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second retrieve for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_retrieval = retrieve(evidence_embeddings, train_claims_embeddings, evidence_df , train_df, 150, 4, True, 2, cross_encoder, True)\n",
        "# save_retrieval(train_retrieval, 'train_retrieval.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dev_retrieval = retrieve(evidence_embeddings, dev_claims_embeddings, evidence_df , dev_df, 150, 5, True, 0, cross_encoder, True)\n",
        "\n",
        "save_retrieval(dev_retrieval, 'dev_retrieval.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('test-claims-unlabelled.json', 'r') as f:\n",
        "    test_df = json.load(f)\n",
        "flat_list = []\n",
        "for key in test_df:\n",
        "\n",
        "    flat_list.append({\"ID\": key, \"claim_text\": test_df[key]['claim_text']})\n",
        "tst_df = pd.DataFrame(flat_list)\n",
        "tst_claims_embeddings = model.encode(tst_df['claim_text'], batch_size=64, show_progress_bar=True)\n",
        "retrieval = retrieve(evidence_embeddings, tst_claims_embeddings, evidence_df , tst_df, 150, 5, False, 0, cross_encoder, False)\n",
        "save_retrieval(retrieval, 'test-output.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
